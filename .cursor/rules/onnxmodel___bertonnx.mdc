---
description: Describes letsearch's ONNX model interface (ONNXModel) and BertONNX implementation for BERT models using ONNX Runtime and tokenizers.
globs: src/model/backends/onnx/bert_onnx.rs
alwaysApply: false
---
# Chapter 7: ONNXModel / BertONNX

In [Chapter 6: VectorIndex](vectorindex.mdc), we saw how vector embeddings are stored and searched efficiently. Those embeddings are generated by models managed by the [ModelManager](modelmanager.mdc). This chapter delves into the specifics of how `letsearch` interacts with these models, focusing on the interface for ONNX models (`ONNXModelTrait` and `ONNXModel`) and its concrete implementation for BERT-style transformers, `BertONNX`.

## Motivation

Interacting directly with machine learning runtimes like ONNX Runtime (`ort`) and associated libraries like `tokenizers` involves significant complexity. This includes:
*   Loading and configuring the runtime environment (e.g., setting up CPU or CUDA execution providers).
*   Loading the specific model graph (`.onnx` file) and tokenizer configuration (`tokenizer.json`).
*   Preprocessing input text: tokenizing, padding to uniform lengths, generating attention masks, and handling token type IDs if required by the model.
*   Converting preprocessed inputs into the tensor format expected by the runtime (`ndarray`).
*   Executing the model inference.
*   Extracting the desired output (e.g., embeddings) from the model's potentially complex output structure and converting it to the correct data type (`f16`, `f32`).

The `ONNXModel` trait and the `BertONNX` struct abstract away these complexities. `ONNXModelTrait` defines a standard interface for any ONNX model within `letsearch`, allowing the [ModelManager](modelmanager.mdc) to treat different models polymorphically. `BertONNX` provides a concrete implementation for BERT-like transformer models, encapsulating all the steps mentioned above using the `ort` and `tokenizers` crates. This separation keeps the core application logic clean and independent of the specific model execution details.

## Central Use Case: Generating Embeddings with `BertONNX`

Imagine the [ModelManager](modelmanager.mdc) needs to generate embeddings for a batch of sentences using a loaded BERT model (which happens to be an instance of `BertONNX` accessed via the `ONNXModel` trait).

1.  **Request:** The `ModelManager` holds an `Arc<RwLock<dyn ONNXModel>>` identified by `model_id`. It calls `predict(model_id, vec!["sentence one", "sentence two"])`.
2.  **Lookup & Lock:** `ModelManager` looks up the `Arc` by `model_id`, acquires a read lock on the `RwLock` containing the `BertONNX` instance.
3.  **Metadata Check:** `ModelManager` might first call `output_dtype(model_id)` on the trait object to determine if the model produces `f16` or `f32` embeddings. Let's assume it's `f32`.
4.  **Delegate Prediction:** `ModelManager` calls `predict_f32(texts)` on the `BertONNX` instance (via the trait object).
5.  **`BertONNX` Execution:**
    *   Tokenizes `"sentence one"` and `"sentence two"` using its internal `tokenizers::Tokenizer`.
    *   Pads the token sequences and creates attention masks (and token type IDs if needed).
    *   Converts token IDs, masks, etc., into `ndarray::Array2<i64>`.
    *   Calls `ort::Session::run` with these input tensors.
    *   Extracts the output embeddings tensor.
    *   Converts the output tensor to `ndarray::Array2<f32>`.
6.  **Return:** `BertONNX` returns `Ok(Arc<Array2<f32>>)`.
7.  **Wrap & Release:** `ModelManager` receives the `Arc<Array2<f32>>`, wraps it in `Embeddings::F32`, releases the read lock, and returns the `Embeddings` enum.

## Core Concepts

1.  **`ONNXModelTrait` / `ONNXModel` Interface (`src/model/model_utils.rs`):**
    *   This trait defines the essential contract for any model implementation intended to run via ONNX Runtime within `letsearch`. It inherits from a base `ModelTrait` (defining the `new` constructor).
    *   Key methods:
        *   `output_dtype()`: Returns the expected embedding data type (`ModelOutputDType::F16` or `F32`).
        *   `output_dim()`: Returns the dimensionality (embedding size) of the output vector.
        *   `predict_f16(texts)`: Takes input texts and returns `f16` embeddings.
        *   `predict_f32(texts)`: Takes input texts and returns `f32` embeddings.
    *   The `ONNXModel` type alias (`pub trait ONNXModel: ModelTrait + ONNXModelTrait + Send + Sync {}`) combines these traits and adds `Send + Sync` bounds, making implementors thread-safe and usable across async tasks.
    *   This interface allows the [ModelManager](modelmanager.mdc) to manage and use different ONNX model architectures without knowing their specific implementation details.

    ```rust
    // src/model/model_utils.rs (Simplified Trait Definition)
    use anyhow;
    use async_trait::async_trait;
    use half::f16;
    use ndarray::Array2;
    use std::sync::Arc;

    #[async_trait]
    pub trait ModelTrait { // Base trait
        async fn new(model_dir: &str, model_file: &str) -> anyhow::Result<Self> where Self: Sized;
    }

    #[async_trait]
    pub trait ONNXModelTrait: ModelTrait { // ONNX-specific methods
        async fn output_dtype(&self) -> anyhow::Result<ModelOutputDType>;
        async fn output_dim(&self) -> anyhow::Result<i64>;
        async fn predict_f16(&self, texts: Vec<&str>) -> anyhow::Result<Arc<Array2<f16>>>;
        async fn predict_f32(&self, texts: Vec<&str>) -> anyhow::Result<Arc<Array2<f32>>>;
    }

    // Combine traits and add thread-safety bounds
    pub trait ONNXModel: ModelTrait + ONNXModelTrait + Send + Sync {}
    impl<T> ONNXModel for T where T: ModelTrait + ONNXModelTrait + Send + Sync {}
    ```
    *   **Explanation:** Defines the standard interface (`ONNXModelTrait`) and the combined, thread-safe trait (`ONNXModel`) used by the `ModelManager`.

2.  **`BertONNX` Implementation (`src/model/backends/onnx/bert_onnx.rs`):**
    *   **Initialization (`new`):**
        *   Uses `std::sync::Once` to initialize the global `ort` environment safely (setting up execution providers like CPU or CUDA if the `cuda` feature is enabled).
        *   Loads the ONNX model file using `ort::Session::builder()`, configuring optimization levels and threading.
        *   Loads the tokenizer configuration from `tokenizer.json` using `tokenizers::Tokenizer::from_file`. Configures padding strategy (`PaddingParams`).
        *   Inspects the loaded `ort::Session`'s input and output metadata to determine the `output_dim` (embedding size), `output_dtype` (`f16` or `f32`), and whether the model requires `token_type_ids` as input.
        *   Stores the `ort::Session` and `tokenizers::Tokenizer` (wrapped in `Arc` for sharing) along with the extracted metadata.

    ```rust
    // src/model/backends/onnx/bert_onnx.rs (Simplified new)
    use ort::{Session, GraphOptimizationLevel, CPUExecutionProvider};
    #[cfg(feature = "cuda")]
    use ort::CUDAExecutionProvider;
    use tokenizers::{Tokenizer, PaddingParams};
    use std::sync::{Arc, Once};
    use std::path::Path;
    use async_trait::async_trait;

    static ORT_INIT: Once = Once::new(); // Global ONNX Runtime init flag

    pub struct BertONNX {
        pub model: Arc<Session>,
        pub tokenizer: Arc<Tokenizer>,
        output_dtype: ModelOutputDType,
        output_dim: i64,
        needs_token_type_ids: bool,
    }

    #[async_trait]
    impl ModelTrait for BertONNX {
        async fn new(model_dir: &str, model_file: &str) -> anyhow::Result<Self> {
            ORT_INIT.call_once(|| { // Initialize ORT environment once
                ort::init().with_execution_providers([
                    // Conditionally include CUDA
                    #[cfg(feature = "cuda")] CUDAExecutionProvider::default().build(),
                    CPUExecutionProvider::default().build(),
                ]).commit().expect("ORT init failed");
            });

            let model_path = Path::new(model_dir).join(model_file);
            let session = Session::builder()?
                .with_optimization_level(GraphOptimizationLevel::Level3)?
                // ... other configs ...
                .commit_from_file(model_path)?;

            let tokenizer_path = Path::new(model_dir).join("tokenizer.json");
            let mut tokenizer = Tokenizer::from_file(tokenizer_path)?;
            tokenizer.with_padding(/* ... PaddingParams ... */);

            // --- Extract metadata from session.inputs / session.outputs ---
            let output_dim = /* ... logic to get dim from session.outputs[1] ... */;
            let output_dtype = /* ... logic to get dtype from session.outputs[1] ... */;
            let needs_token_type_ids = /* ... check session.inputs for "token_type_ids" ... */;

            Ok(Self {
                model: Arc::new(session),
                tokenizer: Arc::new(tokenizer),
                output_dim, output_dtype, needs_token_type_ids
            })
        }
    }
    ```
    *   **Explanation:** Shows the one-time ORT initialization, loading the session and tokenizer, configuring padding, and extracting necessary metadata from the loaded model.

    *   **Tokenization (`tokenizers` crate):**
        *   The `predict_fXX` methods use the stored `tokenizer` (`Arc<Tokenizer>`).
        *   `tokenizer.encode_batch(texts, true)` tokenizes multiple input texts concurrently. The `true` enables added special tokens (like `[CLS]`, `[SEP]`).
        *   Padding is automatically applied based on the `PaddingParams` set during initialization.
        *   `encoding.get_ids()`, `encoding.get_attention_mask()`, and `encoding.get_type_ids()` are used to extract the numerical data needed for the model.

    *   **Tensor Preparation (`ndarray`):**
        *   The extracted IDs, masks, and type IDs (which are typically `Vec<u32>` or similar) are flattened and collected into `Vec<i64>`.
        *   These flat vectors are then reshaped into 2D arrays (`ndarray::Array2<i64>`) with dimensions `[batch_size, sequence_length]` using `Array2::from_shape_vec`.

    *   **ONNX Runtime Inference (`ort` crate):**
        *   The `ndarray` tensors are passed to the model using `ort::inputs!{...}` macro, which requires owned arrays.
        *   `model.run(...)` executes the inference. `model` here is the `Arc<Session>`.
        *   The result is an `OrtOwnedOutputs`, a vector-like structure holding output tensors.
        *   `outputs[1]` (assuming the desired embedding is the second output, often the case for pooled or last hidden state) is accessed.
        *   `try_extract_tensor::<f16>()` or `try_extract_tensor::<f32>()` attempts to extract the tensor data with the expected type.
        *   `.into_dimensionality::<Ix2>()` converts the potentially higher-dimensional tensor into a 2D array (`[batch_size, embedding_dim]`).

    *   **Asynchronous Execution (`tokio::task::spawn_blocking`):**
        *   Both tokenization and ONNX inference can be CPU-intensive and block the current thread.
        *   To prevent blocking the asynchronous Tokio runtime (which handles network requests, file I/O, etc.), these potentially blocking operations are wrapped in `tokio::task::spawn_blocking`. This moves the computation to a dedicated blocking thread pool managed by Tokio, allowing the async runtime to continue processing other tasks concurrently.

## Using the Abstraction

The [ModelManager](modelmanager.mdc) solely interacts with `BertONNX` (and potentially other future ONNX model implementations) through the `ONNXModel` trait interface. Refer back to [Chapter 5: ModelManager](modelmanager.mdc) (Code Snippets section) to see how `ModelManager::predict`, `predict_f16`, `predict_f32`, `output_dim`, and `output_dtype` look up the model by ID, acquire a read lock, and delegate the call to the corresponding trait method on the `dyn ONNXModel` object. This ensures loose coupling between the model management layer and the specific model execution backends.

## Internal Implementation (`predict_f32` Example)

Let's trace the execution flow inside `BertONNX::predict_f32`.

1.  **Assert Type:** Check if `self.output_dtype` matches `ModelOutputDType::F32`.
2.  **Prepare Inputs:** Clone `Arc`s for `model` and `tokenizer`. Clone input `texts`. Get `needs_token_type_ids` flag.
3.  **Spawn Tokenization:**
    *   Start a `tokio::task::spawn_blocking` closure.
    *   Inside: Call `tokenizer.encode_batch(inputs, true)`.
    *   Extract IDs, masks, and type IDs (if needed) from the encodings.
    *   Create `ndarray::Array2<i64>` for `a_ids`, `a_mask`, and optionally `a_t_ids`.
    *   Return these arrays (`(a_ids, a_mask, a_t_ids)`).
4.  **Await Tokenization:** `await` the result from `spawn_blocking`.
5.  **Spawn Inference:**
    *   Start another `tokio::task::spawn_blocking` closure, moving the `model` Arc and the tokenized tensors (`a_ids`, `a_mask`, `a_t_ids`) into it.
    *   Inside: Call `model.run(ort::inputs![...])` using the correct combination of tensors based on `needs_token_type_ids`.
    *   Extract the output tensor (e.g., `outputs[1]`).
    *   Use `try_extract_tensor::<f32>()` and `.into_dimensionality::<Ix2>()`.
    *   Return the resulting `Array2<f32>`.
6.  **Await Inference:** `await` the result (the embeddings tensor) from `spawn_blocking`.
7.  **Wrap & Return:** Wrap the owned `Array2<f32>` in `Arc::new()` and return `Ok(arc)`.

```mermaid
sequenceDiagram
    participant MM as ModelManager
    participant BONNX as BertONNX (dyn ONNXModel)
    participant SpawnBlockTok as tokio::spawn_blocking (Tokenize)
    participant Tokenizer as tokenizers::Tokenizer
    participant SpawnBlockInf as tokio::spawn_blocking (Inference)
    participant Session as ort::Session

    MM->>+BONNX: predict_f32(texts)
    BONNX->>BONNX: Clone Arcs (model, tokenizer)
    BONNX->>+SpawnBlockTok: Spawn closure(texts, tokenizer)
    SpawnBlockTok->>+Tokenizer: encode_batch(texts)
    Tokenizer-->>-SpawnBlockTok: encodings
    SpawnBlockTok->>SpawnBlockTok: Extract IDs, masks, type IDs
    SpawnBlockTok->>SpawnBlockTok: Create ndarray Tensors (ids, mask, type_ids)
    SpawnBlockTok-->>-BONNX: Returns Tensors (ids, mask, type_ids)
    BONNX->>+SpawnBlockInf: Spawn closure(model, tensors)
    SpawnBlockInf->>+Session: run(ort::inputs![tensors])
    Session-->>-SpawnBlockInf: OrtOwnedOutputs
    SpawnBlockInf->>SpawnBlockInf: Extract & Convert output[1] to Array2<f32>
    SpawnBlockInf-->>-BONNX: Returns embeddings_tensor: Array2<f32>
    BONNX->>BONNX: Wrap tensor in Arc
    BONNX-->>-MM: Ok(Arc<Array2<f32>>)
```

```rust
// src/model/backends/onnx/bert_onnx.rs (Simplified predict_f32)
use ndarray::{Array2, Ix2};
use half::f16; // Used in predict_f16, imported here for context
use tokio::task;
use rayon::prelude::*; // Used for parallel processing inside tokenization

#[async_trait]
impl ONNXModelTrait for BertONNX {
    // --- predict_f16 is analogous, using f16 types ---

    async fn predict_f32(&self, texts: Vec<&str>) -> anyhow::Result<Arc<Array2<f32>>> {
        assert_eq!(self.output_dtype().await?, ModelOutputDType::F32);

        let inputs: Vec<String> = texts.par_iter().map(|s| s.to_string()).collect();
        let model = self.model.clone(); // Clone Arc for moving
        let tokenizer = self.tokenizer.clone(); // Clone Arc for moving
        let needs_token_type_ids = self.needs_token_type_ids;

        // --- 1. Tokenize in blocking thread ---
        let (a_ids, a_mask, a_t_ids) = task::spawn_blocking(move || {
            let encodings = tokenizer.encode_batch(inputs, true).unwrap();
            let seq_len = encodings[0].len();
            let batch_size = encodings.len();

            // Extract into flat Vec<i64> (parallelized internally by rayon)
            let ids: Vec<i64> = /* ... extract IDs ... */ Vec::new();
            let mask: Vec<i64> = /* ... extract masks ... */ Vec::new();
            // Reshape into Array2<i64>
            let a_ids = Array2::from_shape_vec((batch_size, seq_len), ids).unwrap();
            let a_mask = Array2::from_shape_vec((batch_size, seq_len), mask).unwrap();

            let a_t_ids = if needs_token_type_ids {
               let t_ids: Vec<i64> = /* ... extract type IDs ... */ Vec::new();
               Some(Array2::from_shape_vec((batch_size, seq_len), t_ids).unwrap())
            } else { None };

            (a_ids, a_mask, a_t_ids) // Return tensors
        }).await?;

        // --- 2. Run model inference in blocking thread ---
        let embeddings_tensor = task::spawn_blocking(move || {
            // Construct ort::inputs based on whether a_t_ids is Some
            let outputs = if let Some(tti) = a_t_ids {
                model.run(ort::inputs![a_ids, tti, a_mask].unwrap())
            } else {
                model.run(ort::inputs![a_ids, a_mask].unwrap())
            }.unwrap();

            // Extract the specific output tensor (e.g., index 1)
            outputs[1].try_extract_tensor::<f32>().unwrap()
                .into_dimensionality::<Ix2>().unwrap().to_owned()
        }).await?;

        Ok(Arc::new(embeddings_tensor))
    }

    async fn output_dtype(&self) -> anyhow::Result<ModelOutputDType> { Ok(self.output_dtype.clone()) }
    async fn output_dim(&self) -> anyhow::Result<i64> { Ok(self.output_dim) }
}
```
*   **Explanation:** This simplified code illustrates the two main `spawn_blocking` calls: one for tokenization/tensor creation and one for ONNX model execution. It shows how inputs are prepared for `ort::Session::run` and how the output tensor is extracted and typed.

## Conclusion

The `ONNXModel` trait and the `BertONNX` implementation provide a robust and abstracted way to handle ONNX model inference within `letsearch`. They encapsulate the complexities of tokenization, tensor manipulation, and interaction with the ONNX Runtime, using asynchronous patterns (`spawn_blocking`) to maintain application responsiveness. This allows the rest of `letsearch`, particularly the [ModelManager](modelmanager.mdc), to work with models at a higher level, focusing on loading and orchestrating predictions rather than backend-specific details.

The final chapter, [hf_ops](hf_ops.mdc), will cover the utility functions used by the `ModelManager` to download model files (like the `.onnx` graph and `tokenizer.json` used by `BertONNX`) from the Hugging Face Hub.


---

Generated by [Rules for AI](https://github.com/altaidevorg/rules-for-ai)